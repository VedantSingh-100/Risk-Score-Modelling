# Parshvi Risk Model

Production-grade repository for building labels, engineering features, and training binary risk models using baseline and gradient boosting approaches.

## ğŸ¯ Project Overview

This repository contains a complete machine learning pipeline for risk modeling, including:
- **Label Engineering**: Union-based target construction from multiple sources
- **Feature Engineering**: Transform-based preprocessing with redundancy pruning
- **Model Training**: Baseline (Logistic + GBDT) and advanced boosting models
- **Quality Control**: Comprehensive validation and leakage detection

## ğŸ“Š Key Results

- **Target**: `label_union` with prevalence â‰ˆ 0.52 (25,695 positives from 49,389 samples)
- **Features**: 39 final features (after redundancy pruning from 56 initial features)
- **Performance**: XGBoost CV AUC ~0.868, AP ~0.884
- **Stability**: High feature consistency across CV folds

## ğŸ—ï¸ Repository Structure

```
parshvi-risk-model/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ training_config.yaml    # Training parameters
â”‚   â”œâ”€â”€ guard_Set.txt          # Features to exclude
â”‚   â””â”€â”€ Selected_label_sources.csv # Label source configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw data files (not tracked)
â”‚   â”œâ”€â”€ interim/               # Intermediate processing
â”‚   â””â”€â”€ processed/             # Clean, processed data
â”‚       â”œâ”€â”€ X_features.parquet # Feature matrix
â”‚       â””â”€â”€ y_label.csv        # Target labels
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train_baselines.py # Logistic + GBDT training
â”‚   â”‚   â”œâ”€â”€ train_boosters.py  # XGBoost/LightGBM training
â”‚   â”‚   â””â”€â”€ train_advanced.py  # Original advanced training script
â”‚   â”œâ”€â”€ preprocessing/         # Data preprocessing modules
â”‚   â””â”€â”€ utils/                 # Utility functions
â”œâ”€â”€ scripts/                   # Execution scripts
â”‚   â””â”€â”€ repo_relayout.sh      # Repository organization script
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ PIPELINE.md           # Detailed pipeline documentation
â”‚   â””â”€â”€ *.md                  # Other documentation files
â””â”€â”€ artifacts/                 # Generated outputs (not tracked)
    â”œâ”€â”€ models/               # Trained model files
    â”œâ”€â”€ reports/              # Training reports and metrics
    â””â”€â”€ logs/                 # Execution logs
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd parshvi-risk-model

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Training

**Baseline Models (Logistic + GBDT):**
```bash
python -m src.models.train_baselines \
    --data-dir data/processed \
    --config-dir configs \
    --out-dir artifacts/reports/baselines
```

**Boosting Models (XGBoost/LightGBM):**
```bash
python -m src.models.train_boosters \
    --data-dir data/processed \
    --out-dir artifacts/reports/boosters \
    --redundancy-r 0.97
```

### 3. View Results

Results are saved in `artifacts/reports/` with:
- Cross-validation metrics
- Feature importance rankings
- Out-of-fold predictions
- Model performance summaries

## ğŸ“‹ Pipeline Overview

### Labels
- **Target**: `label_union` constructed from 9 eligible sources
- **Strategy**: Lifetime signals included with deduplication
- **Quality**: Dominance â‰¥ 0.6, quality â‰¥ 0.6, Jaccard â‰¤ 0.85

### Features
- **Selection**: Fill rate â‰¥ 0.85 from Internal_Algo dictionary
- **Transforms**: log1p for counts, asinh for ratios, robust scaling
- **Guards**: Identifier and outcome-related feature removal
- **Redundancy**: Graph-based pruning of highly correlated features (r â‰¥ 0.97)

### Models
- **Baselines**: Logistic Regression (ElasticNet) + sklearn GradientBoosting
- **Boosters**: XGBoost (preferred) or LightGBM with early stopping
- **Validation**: 5-fold stratified cross-validation
- **Calibration**: Platt scaling for probability calibration

## ğŸ”§ Configuration

Key parameters are controlled via `configs/training_config.yaml`:

```yaml
feature_engineering:
  redundancy_threshold: 0.97
  fill_rate_threshold: 0.85

cross_validation:
  n_splits: 5
  random_seeds: [42, 202, 404, 808, 1337]

models:
  xgboost:
    learning_rate: 0.02
    max_depth: 6
    early_stopping_rounds: 300
```

## ğŸ“ˆ Model Performance

| Model | CV AUC | CV AP | Stability |
|-------|--------|-------|-----------|
| Logistic | ~0.85 | ~0.86 | High |
| GBDT | ~0.86 | ~0.87 | High |
| XGBoost | ~0.87 | ~0.88 | High |

## ğŸ›¡ï¸ Quality Assurance

- **Leakage Detection**: Automated scanning for outcome-related features
- **Guard Lists**: Explicit exclusion of problematic variables
- **Redundancy Control**: Correlation-based feature pruning
- **Null Handling**: Zero null/infinite values after preprocessing
- **Stability Analysis**: Cross-seed feature importance consistency

## ğŸ“š Documentation

- [`docs/PIPELINE.md`](docs/PIPELINE.md) - Detailed pipeline documentation
- [`configs/training_config.yaml`](configs/training_config.yaml) - Training parameters
- Model cards and reports in `docs/` directory

## ğŸ¤ Contributing

1. Follow the existing code structure
2. Update documentation for any changes
3. Run both baseline and booster training for validation
4. Ensure all outputs are generated in `artifacts/`

## ğŸ“„ License

[Add your license information here]

## ğŸ“ Contact

[Add contact information here]
