diff --git a/README.md b/README.md
index 0b525af..cc365aa 100644
--- a/README.md
+++ b/README.md
@@ -1,169 +1,248 @@
-# Parshvi Risk Model
+# Risk Model Pipeline (Snapshot Mode, Leakage-Safe)
+
+This repository builds a productionâ€‘grade **credit risk score** using a fully reproducible pipeline:
+
+1) **Label discovery & composition** from raw vendor variables  
+2) **Leakage guards** and **time-safety** for features  
+3) **Feature engineering** (fill-rate filtering, transforms, encoding, scaling, correlation pruning)  
+4) **Model training** (GBDT sweep, MLP stack, and a deep attention model)  
+5) **Calibration & reporting**
 
-Production-grade repository for building labels, engineering features, and training binary risk models using baseline and gradient boosting approaches.
+The pipeline is designed for **snapshot training with lifetime labels** while **removing outcome-like and lifetime features** from the model inputs to prevent leakage.
 
-## ğŸ¯ Project Overview
+---
 
-This repository contains a complete machine learning pipeline for risk modeling, including:
-- **Label Engineering**: Union-based target construction from multiple sources
-- **Feature Engineering**: Transform-based preprocessing with redundancy pruning
-- **Model Training**: Baseline (Logistic + GBDT) and advanced boosting models
-- **Quality Control**: Comprehensive validation and leakage detection
+## Data Inputs
 
-## ğŸ“Š Key Results
+- `data/raw/50k_users_merged_data_userfile_updated_shopping.csv`  
+  > Wide user-level table (~50k rows x ~1.6k columns). Each column is a raw variable.
 
-- **Target**: `label_union` with prevalence â‰ˆ 0.52 (25,695 positives from 49,389 samples)
-- **Features**: 39 final features (after redundancy pruning from 56 initial features)
-- **Performance**: XGBoost CV AUC ~0.868, AP ~0.884
-- **Stability**: High feature consistency across CV folds
+- `data/raw/Internal_Algo360VariableDictionary_WithExplanation.xlsx`  
+  > Dictionary that maps variable codes to human-readable descriptions. We infer **windows** and **semantics** from these descriptions.
 
-## ğŸ—ï¸ Repository Structure
+- *(Optional)* `data/raw/variable_catalog.csv`  
+  > Additional variable metadata (if available).
 
-```
-parshvi-risk-model/
-â”œâ”€â”€ README.md                    # This file
-â”œâ”€â”€ requirements.txt             # Python dependencies
-â”œâ”€â”€ .gitignore                  # Git ignore rules
-â”œâ”€â”€ configs/                    # Configuration files
-â”‚   â”œâ”€â”€ training_config.yaml    # Training parameters
-â”‚   â”œâ”€â”€ guard_Set.txt          # Features to exclude
-â”‚   â””â”€â”€ Selected_label_sources.csv # Label source configuration
-â”œâ”€â”€ data/
-â”‚   â”œâ”€â”€ raw/                   # Raw data files (not tracked)
-â”‚   â”œâ”€â”€ interim/               # Intermediate processing
-â”‚   â””â”€â”€ processed/             # Clean, processed data
-â”‚       â”œâ”€â”€ X_features.parquet # Feature matrix
-â”‚       â””â”€â”€ y_label.csv        # Target labels
-â”œâ”€â”€ src/                       # Source code
-â”‚   â”œâ”€â”€ models/
-â”‚   â”‚   â”œâ”€â”€ train_baselines.py # Logistic + GBDT training
-â”‚   â”‚   â”œâ”€â”€ train_boosters.py  # XGBoost/LightGBM training
-â”‚   â”‚   â””â”€â”€ train_advanced.py  # Original advanced training script
-â”‚   â”œâ”€â”€ preprocessing/         # Data preprocessing modules
-â”‚   â””â”€â”€ utils/                 # Utility functions
-â”œâ”€â”€ scripts/                   # Execution scripts
-â”‚   â””â”€â”€ repo_relayout.sh      # Repository organization script
-â”œâ”€â”€ docs/                      # Documentation
-â”‚   â”œâ”€â”€ PIPELINE.md           # Detailed pipeline documentation
-â”‚   â””â”€â”€ *.md                  # Other documentation files
-â””â”€â”€ artifacts/                 # Generated outputs (not tracked)
-    â”œâ”€â”€ models/               # Trained model files
-    â”œâ”€â”€ reports/              # Training reports and metrics
-    â””â”€â”€ logs/                 # Execution logs
-```
+---
 
-## ğŸš€ Quick Start
+## Stage 1 â€” Label Discovery & Composition
 
-### 1. Setup Environment
+**File:** `src/smart_variable_framework.py`
 
-```bash
-# Clone the repository
-git clone <repository-url>
-cd parshvi-risk-model
+**What it does**
+- Crawls all columns and descriptions to detect **negative-outcome patterns** via regex:
+  - `default`, `dpd`, `overdue`, `arrear`, `write-off`, `charge-off`, `miss(ed|ing)`, `min_due|mindue`,
+  - `overlimit`, `decline`, `reject`, `insufficient`, `penalty`, `bounced|nsf`, `negative events` â€¦
+- Scores candidates with keyword weights and **data quality checks** (missing %, variance, binary shape, prevalence).
+- Produces **composite labels** from multiple sources:
+  - **Union** (with "severe" weighting threshold)
+  - **Severityâ€‘weighted** (quantileâ€‘thresholded to target prevalence)
+  - **Hierarchical** (worst-first)
+  - **Clustered** (KMeans on source correlation; representative pick)
+- **Duplicate control:** Drop near-duplicate sources by **Jaccard** similarity.
+- **Dominance control:** If one source explains â‰¥ dominance_cutoff of label positives, it can be downâ€‘weighted or pruned (nonâ€‘fatal).
+- **Rescue policy:** If no viable labels surface, progressively relaxes rules (but keeps exposures out).
 
-# Install dependencies
-pip install -r requirements.txt
-```
+**Outputs (stored under `data/interim/`):**
+- `negative_pattern_variables.csv` â€“ All variables that matched negative patterns with stats
+- `smart_label_candidates.csv` â€“ All variables scored for outcomeâ€‘likeness
+- `composite_labels.csv` â€“ Columns: `label_union`, `label_weighted`, `label_hierarchical`, `label_clustered`, â€¦
+- `event_contribution_summary.csv` â€“ Share of each source among positives of the union label
+- `jaccard_matrix.csv` â€“ Overlap matrix among label sources
+- `do_not_use_features.txt` â€“ Baseline **guard** list (label sources, outcome-like columns, families)
+- `smart_framework_report.md` â€“ Human-readable report
+- `recommended_pipeline.json` â€“ Machine recommendations (best label, top features if available)
 
-### 2. Run Training
+**Why lifetime labels?**  
+Your data shows best stability and prevalence with lifetime flags. We keep **lifetime labels**, but **do NOT allow lifetime or outcome-like features** into the model to prevent leakage.
 
-**Baseline Models (Logistic + GBDT):**
-```bash
-python -m src.models.train_baselines \
-    --data-dir data/processed \
-    --config-dir configs \
-    --out-dir artifacts/reports/baselines
-```
+---
 
-**Boosting Models (XGBoost/LightGBM):**
-```bash
-python -m src.models.train_boosters \
-    --data-dir data/processed \
-    --out-dir artifacts/reports/boosters \
-    --redundancy-r 0.97
-```
+## Stage 2 â€” Leakage Guard & Time Safety
 
-### 3. View Results
+We create a **guard set** of columns that must never enter the feature matrix:
 
-Results are saved in `artifacts/reports/` with:
-- Cross-validation metrics
-- Feature importance rankings
-- Out-of-fold predictions
-- Model performance summaries
+- **Label sources** used to build the target  
+- Any column whose **name/description matches outcome patterns** (same regex as above)  
+- **Whole families** of the label sources (e.g., `var201xxx`)  
+- **ID-like** columns (e.g., `*_id`, `email`, `mobile`, `pan`, etc.)  
+- **Near-unique** columns (â‰¥ 98% unique values)  
+- **Too-predictive smell test** (any single raw feature with suspiciously high AUC on the label)  
+- **Lifetime-window features** when labels are lifetime (to remove lookahead leakage)
 
-## ğŸ“‹ Pipeline Overview
+This guard is written to `data/interim/do_not_use_features.txt` and is respected downstream.
 
-### Labels
-- **Target**: `label_union` constructed from 9 eligible sources
-- **Strategy**: Lifetime signals included with deduplication
-- **Quality**: Dominance â‰¥ 0.6, quality â‰¥ 0.6, Jaccard â‰¤ 0.85
+---
 
-### Features
-- **Selection**: Fill rate â‰¥ 0.85 from Internal_Algo dictionary
-- **Transforms**: log1p for counts, asinh for ratios, robust scaling
-- **Guards**: Identifier and outcome-related feature removal
-- **Redundancy**: Graph-based pruning of highly correlated features (r â‰¥ 0.97)
+## Stage 3 â€” Feature Engineering (Snapshot)
 
-### Models
-- **Baselines**: Logistic Regression (ElasticNet) + sklearn GradientBoosting
-- **Boosters**: XGBoost (preferred) or LightGBM with early stopping
-- **Validation**: 5-fold stratified cross-validation
-- **Calibration**: Platt scaling for probability calibration
+**File:** `src/build_features_snapshot.py`
 
-## ğŸ”§ Configuration
+**What it does**
+1. **Pick a label** from `composite_labels.csv` (default: `label_union`) and write `data/processed/y_label.csv`.
+2. Build the **candidate feature set** = all columns minus the **guard** list.
+3. **Fillâ€‘rate filter:** keep columns with fillâ€‘rate â‰¥ **0.85** (i.e., â‰¤ 15% missing).
+4. **Categorical encoding:**  
+   - **OHE** if â‰¤ 8 distinct values  
+   - **Frequency encoding** otherwise
+5. **Numeric transforms:**  
+   - `log1p` for **counts/occurrences**  
+   - `asinh` (safe for negatives) for **ratios/amounts/other**  
+   - **median imputation** for NaNs
+6. **Robust scaling** (median/IQR) for numerics
+7. **Graph-based correlation pruning** at **|r| â‰¥ 0.97** with **qualityâ€‘score/variance priority**
+8. Write final matrix to `data/processed/X_features.parquet` + manifests:
+   - `feature_keep_list.txt` â€“ final kept features  
+   - `feature_drop_corr.txt` â€“ correlated drops  
+   - `fe_manifest.json` â€“ full FE metadata
 
-Key parameters are controlled via `configs/training_config.yaml`:
+**Why these choices**
+- High **fillâ€‘rate** ensures stable features.  
+- `log1p` and `asinh` tame heavy tails and preserve directionality.  
+- **Robust scaling** protects against outliers.  
+- **Graphâ€‘pruning** reduces redundancy and overfitting.
 
-```yaml
-feature_engineering:
-  redundancy_threshold: 0.97
-  fill_rate_threshold: 0.85
+---
 
-cross_validation:
-  n_splits: 5
-  random_seeds: [42, 202, 404, 808, 1337]
+## Stage 4 â€” Modeling
 
-models:
-  xgboost:
-    learning_rate: 0.02
-    max_depth: 6
-    early_stopping_rounds: 300
-```
+### (A) Gradient Boosted Trees (XGBoost / LightGBM)
+
+**File:** `src/train_gbdt.py`
 
-## ğŸ“ˆ Model Performance
+- **Stratified Kâ€‘fold OOF** (default 5â€‘fold)
+- **Random hyperparameter sweep**
+- **Early stopping** on `auc` or `aucpr` (auto picks `aucpr` if prevalence â‰¤ 20%)
+- Respects `feature_keep_list.txt` (or uses `feature_drop_corr.txt` / `--prune-corr`)
+- Optionally **monotone constraints** (`data/processed/monotone_config.json`)
 
-| Model | CV AUC | CV AP | Stability |
-|-------|--------|-------|-----------|
-| Logistic | ~0.85 | ~0.86 | High |
-| GBDT | ~0.86 | ~0.87 | High |
-| XGBoost | ~0.87 | ~0.88 | High |
+**Artifacts**
+- `best_params.json`, `trials_summary.csv`  
+- `oof_running_best.csv` (OOF predictions & y)  
+- `deciles_running_best.csv`, `feature_importance_running_best.csv`  
+- **Platt calibration** summary + `deciles_running_best_calibrated.csv`
 
-## ğŸ›¡ï¸ Quality Assurance
+---
 
-- **Leakage Detection**: Automated scanning for outcome-related features
-- **Guard Lists**: Explicit exclusion of problematic variables
-- **Redundancy Control**: Correlation-based feature pruning
-- **Null Handling**: Zero null/infinite values after preprocessing
-- **Stability Analysis**: Cross-seed feature importance consistency
+### (B) Linear Stacking (XGB + MLP)
 
-## ğŸ“š Documentation
+**File:** `src/train_mlp_stack.py`
 
-- [`docs/PIPELINE.md`](docs/PIPELINE.md) - Detailed pipeline documentation
-- [`configs/training_config.yaml`](configs/training_config.yaml) - Training parameters
-- Model cards and reports in `docs/` directory
+- Gets OOF predictions from an **XGB** (using best params) and a **baseline MLP**
+- **Stacker** = Logistic regression (Platt), with optional **nested OOF** (`--stack-nested`) for a fully unbiased stacked OOF.
+- Artifacts: `summary.csv`, `oof_predictions.csv`, `deciles_xxx.csv`, `stacker_logit.json`
 
-## ğŸ¤ Contributing
+---
 
-1. Follow the existing code structure
-2. Update documentation for any changes
-3. Run both baseline and booster training for validation
-4. Ensure all outputs are generated in `artifacts/`
+### (C) Deep Attention Model (FTâ€‘Transformer for Tabular)
 
-## ğŸ“„ License
+**File:** `src/train_fttransformer.py` *(see below for code)*
 
-[Add your license information here]
+- **Feature tokenizer** turns each numeric feature into a token (learned perâ€‘feature projection)
+- Several **Transformer blocks** (Multiâ€‘head selfâ€‘attention + PreNorm + residual MLP)
+- **CLS token** + pooling for classification
+- **Cosine LR** with warmup, **AdamW**, **label imbalance** via `pos_weight`
+- **Mixed precision** (if CUDA), **early stopping** on AUC
+- Produces OOF, deciles, and a saved model state
 
-## ğŸ“ Contact
+---
 
-[Add contact information here]
+## Stage 5 â€” Calibration & Reporting
+
+- GBDT: Platt calibration (`oof_cal`)  
+- Deciles and KS/Gini in each model output  
+- W&B logging optional
+
+---
+
+## Directory Layout
+
+```
+data/
+  raw/
+    50k_users_merged_data_userfile_updated_shopping.csv
+    Internal_Algo360VariableDictionary_WithExplanation.xlsx
+  interim/
+    composite_labels.csv
+    do_not_use_features.txt
+    negative_pattern_variables.csv
+    smart_label_candidates.csv
+    smart_framework_report.md
+    ...
+  processed/
+    X_features.parquet
+    y_label.csv
+    feature_keep_list.txt
+    feature_drop_corr.txt
+    fe_manifest.json
+
+model_outputs/
+  gbdt_sweep_YYYYMMDD_HHMMSS/
+  stack_YYYYMMDD_HHMMSS/
+  fttr_YYYYMMDD_HHMMSS/
+  ...
+```
 
+---
+
+## How to Run (Snapshot Mode)
+
+1. **Label discovery & guards**
+   ```bash
+   python -m src.smart_variable_framework
+   # artifacts written to data/interim/
+   ```
+
+2. **Feature engineering (leakage-safe)**
+   ```bash
+   python -m src.build_features_snapshot \
+     --raw-csv   data/raw/50k_users_merged_data_userfile_updated_shopping.csv \
+     --dict-xlsx data/raw/Internal_Algo360VariableDictionary_WithExplanation.xlsx \
+     --interim-dir data/interim \
+     --out-dir     data/processed \
+     --label-column label_union \
+     --fill-rate-min 0.85 --corr-thr 0.97
+   ```
+
+3. **GBDT sweep**
+   ```bash
+   python -m src.train_gbdt \
+     --data-root data/processed \
+     --out-dir   model_outputs/gbdt_sweep_$(date +%Y%m%d_%H%M%S) \
+     --algo xgb --trials 120 --n-splits 5 --eval-metric auto --use-monotone
+   ```
+
+4. **Stacking**
+   ```bash
+   python -m src.train_mlp_stack \
+     --data-root data/processed \
+     --out-dir   model_outputs/stack_$(date +%Y%m%d_%H%M%S) \
+     --best-xgb-params model_outputs/gbdt_sweep_.../best_params.json \
+     --stack-nested
+   ```
+
+5. **Deep attention model (FTâ€‘Transformer)**
+   ```bash
+   python -m src.train_fttransformer \
+     --data-root data/processed \
+     --out-dir   model_outputs/fttr_$(date +%Y%m%d_%H%M%S) \
+     --layers 4 --d-model 96 --heads 8 --dropout 0.2
+   ```
+
+---
+
+## Sanity Checks
+
+- **Leakage guard applied?** `data/interim/do_not_use_features.txt` exists and training logs show it was loaded/applied.
+- **Fillâ€‘rate policy enforced?** FE logs show â‰¥0.85 fillâ€‘rate filtering counts.
+- **Correlation pruning applied?** FE logs report how many features were dropped (r â‰¥ 0.97).
+- **Label prevalence** printed at train start.
+- **OOF metrics** reported; use deciles & KS for business sanity.
+
+---
+
+## Extensibility
+
+- **Time-based splits** (e.g., Janâ€“Sep train, Octâ€“Dec validate): add a `ref_date` column and filter/roll your windows; the framework already isolates lifetime features out of X to remain leakageâ€‘safe even on snapshot data.
+- **Fairness & bias checks**: slice metrics by demographics (if legally permissible).
+- **Monitoring**: rebuild label sources and guard set regularly to catch vendor schema drift.
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 71175ba..576f259 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250914_205025-n57jgfew
\ No newline at end of file
+run-20250915_001523-9udibvt6
\ No newline at end of file
